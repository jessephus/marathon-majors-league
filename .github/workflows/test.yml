name: Test Suite

on:
  pull_request:
    paths:
      - '**.js'
      - '**.ts'
      - '**.tsx'
      - 'package.json'
      - 'package-lock.json'
      - 'tests/**'
      - '.github/workflows/test.yml'
  push:
    branches: [ main ]
    paths:
      - '**.js'
      - '**.ts'
      - '**.tsx'
      - 'package.json'
      - 'package-lock.json'
      - 'tests/**'
      - '.github/workflows/test.yml'
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write

jobs:
  test:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
      
      - name: Install dependencies
        run: npm install
      
      # State Manager Tests (no server required - uses tsx for TypeScript execution)
      - name: Run state manager unit tests
        id: test-state-unit
        run: npm run test:state
        continue-on-error: true
      
      - name: Run state manager integration tests
        id: test-state-integration
        run: npm run test:state:integration
        continue-on-error: true
      
      # Draft Feature Tests (no server required)
      - name: Run draft validation tests
        id: test-draft
        run: npm run test:draft
        continue-on-error: true
      
      # API Client Tests (no server required)
      - name: Run API client tests
        id: test-api-client
        run: npm run test:api-client
        continue-on-error: true
      
      - name: Run SSR integration tests
        id: test-ssr
        run: npm run test:ssr
        continue-on-error: true
      
      - name: Run landing page SSR tests
        id: test-ssr-landing
        run: npm run test:ssr:landing
        continue-on-error: true
      
      - name: Run team session SSR tests
        id: test-ssr-team
        run: npm run test:ssr:team
        continue-on-error: true
      
      # Formatting Utils Tests (no server required)
      - name: Run formatting utilities tests
        id: test-formatting
        run: npm run test:formatting
        continue-on-error: true
      
      # Dynamic Imports Tests (no server required)
      - name: Run dynamic imports tests
        id: test-dynamic
        run: npm run test:dynamic
        continue-on-error: true
      
      - name: Run dynamic imports E2E tests  
        id: test-dynamic-e2e
        run: npm run test:dynamic:e2e
        continue-on-error: true
      
      # Performance Instrumentation Tests (no server required)
      - name: Run performance instrumentation tests
        id: test-perf-instrumentation
        run: npm run test:perf-instrumentation
        continue-on-error: true
      
      - name: Build application
        run: npm run build
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
      
      - name: Start server in background
        run: |
          PORT=3000 npm start > server.log 2>&1 &
          echo $! > server.pid
          echo "Server PID: $(cat server.pid)"
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          PORT: 3000
      
      - name: Wait for server to be ready
        run: |
          echo "Waiting for server to start..."
          for i in {1..60}; do
            if curl -s http://localhost:3000 > /dev/null 2>&1; then
              echo "‚úÖ Server is ready!"
              curl -I http://localhost:3000
              exit 0
            fi
            echo "Attempt $i/60: Server not ready yet..."
            sleep 2
          done
          echo "‚ùå Server failed to start within timeout"
          echo "Server log:"
          cat server.log || echo "No server log found"
          exit 1
      
      - name: Run API endpoint tests
        id: test-api
        run: npm run test:api
        continue-on-error: true
        env:
          TEST_URL: http://localhost:3000
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
      
      - name: Run database tests
        id: test-db
        run: npm run test:db
        continue-on-error: true
        env:
          TEST_URL: http://localhost:3000
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
      
      - name: Run frontend integration tests
        id: test-frontend
        run: npm run test:frontend
        continue-on-error: true
        env:
          TEST_URL: http://localhost:3000
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
      
      - name: Run game flow tests
        id: test-flow
        run: npm run test:flow
        continue-on-error: true
        env:
          TEST_URL: http://localhost:3000
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
      
      - name: Run Next.js routing tests
        id: test-nextjs
        run: npm run test:nextjs
        continue-on-error: true
        env:
          TEST_URL: http://localhost:3000
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
      
      - name: Run salary cap draft tests
        id: test-salarycap
        run: npm run test:salarycap
        continue-on-error: true
        env:
          TEST_URL: http://localhost:3000
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
      
      - name: Run performance benchmarks
        id: test-performance
        run: npm run test:performance
        continue-on-error: true
        env:
          TEST_URL: http://localhost:3000
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
      
      - name: Run legacy regression tests
        id: test-legacy
        run: npm run test:legacy
        continue-on-error: true
        env:
          TEST_URL: http://localhost:3000
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
      
      - name: Stop server
        if: always()
        run: |
          if [ -f server.pid ]; then
            echo "Stopping server (PID: $(cat server.pid))"
            kill $(cat server.pid) || true
            rm server.pid
          fi
          # Also kill any remaining Next.js processes
          pkill -f "next start" || true
      
      - name: Upload test results and logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ github.run_number }}
          path: |
            test-results/
            server.log
            *.log
          retention-days: 30
          if-no-files-found: ignore
      
      - name: Comment test results on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            try {
              console.log('Starting PR comment generation...');
              
              // Get individual test step results
              const testResults = {
                'State Manager Unit Tests': '${{ steps.test-state-unit.outcome }}',
                'State Manager Integration Tests': '${{ steps.test-state-integration.outcome }}',
                'Draft Validation Tests': '${{ steps.test-draft.outcome }}',
                'API Client Tests': '${{ steps.test-api-client.outcome }}',
                'SSR Integration Tests': '${{ steps.test-ssr.outcome }}',
                'Landing Page SSR Tests': '${{ steps.test-ssr-landing.outcome }}',
                'Team Session SSR Tests': '${{ steps.test-ssr-team.outcome }}',
                'Formatting Utilities Tests': '${{ steps.test-formatting.outcome }}',
                'Dynamic Imports Tests': '${{ steps.test-dynamic.outcome }}',
                'Dynamic Imports E2E Tests': '${{ steps.test-dynamic-e2e.outcome }}',
                'Performance Instrumentation Tests': '${{ steps.test-perf-instrumentation.outcome }}',
                'API Endpoint Tests': '${{ steps.test-api.outcome }}',
                'Database Tests': '${{ steps.test-db.outcome }}',
                'Frontend Integration Tests': '${{ steps.test-frontend.outcome }}',
                'Game Flow Tests': '${{ steps.test-flow.outcome }}',
                'Next.js Routing & SSR Tests': '${{ steps.test-nextjs.outcome }}',
                'Salary Cap Draft Tests': '${{ steps.test-salarycap.outcome }}',
                'Performance Benchmarks': '${{ steps.test-performance.outcome }}',
                'Legacy Regression Tests': '${{ steps.test-legacy.outcome }}'
              };
              
              console.log('Test results:', JSON.stringify(testResults, null, 2));
              
              // Build test results list
              let testResultsList = '';
              let failedCount = 0;
              let passedCount = 0;
              
              for (const [name, outcome] of Object.entries(testResults)) {
                if (outcome === 'success') {
                  testResultsList += `- ‚úÖ ${name}\n`;
                  passedCount++;
                } else if (outcome === 'failure') {
                  testResultsList += `- ‚ùå ${name} (FAILED)\n`;
                  failedCount++;
                } else if (outcome === 'skipped') {
                  testResultsList += `- ‚è≠Ô∏è ${name} (Skipped)\n`;
                } else {
                  testResultsList += `- ‚ö†Ô∏è ${name} (${outcome})\n`;
                }
              }
              
              const totalTests = Object.keys(testResults).length;
              const runNumber = '${{ github.run_number }}';
              const runUrl = '${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}';
              const commitSha = '${{ github.sha }}';
              const emoji = failedCount > 0 ? '‚ùå' : 'üß™';
              
              const summary = failedCount > 0
                ? `‚ö†Ô∏è ${failedCount} of ${totalTests} test suites failed. Please review the [workflow logs](${runUrl}) for details.`
                : `üéâ All ${passedCount} test suites passed! Your changes are safe to merge.`;
              
              const testSectionContent = [
                `### ${emoji} Test Suite Results`,
                '',
                `**Status:** ${failedCount > 0 ? 'FAILED' : 'SUCCESS'}`,
                `**Run:** [#${runNumber}](${runUrl})`,
                `**Commit:** ${commitSha}`,
                '',
                `**Results:** ${passedCount}/${totalTests} test suites passed`,
                '',
                '<details>',
                '<summary>View detailed test results</summary>',
                '',
                testResultsList,
                '</details>',
                '',
                summary
              ].join('\n');
              
              console.log('Test section generated, length:', testSectionContent.length);
              
              // Find existing master comment
              console.log('Fetching existing comments...');
              const { data: comments } = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              });
              
              console.log(`Found ${comments.length} existing comments`);
              
              const masterComment = comments.find(comment => 
                comment.user.type === 'Bot' && 
                comment.body.includes('<!-- AUTOMATED_TESTS_MASTER_COMMENT -->')
              );
              
              const testSectionMarker = '<!-- TEST_SUITE_SECTION -->';
              const testSectionEnd = '<!-- TEST_SUITE_SECTION_END -->';
              
              if (masterComment) {
                console.log(`Updating existing master comment ID: ${masterComment.id}`);
                let updatedBody = masterComment.body;
                
                // Replace test section if it exists, otherwise append
                const testSectionRegex = new RegExp(
                  `${testSectionMarker}[\\s\\S]*?${testSectionEnd}`,
                  'g'
                );
                
                if (updatedBody.includes(testSectionMarker)) {
                  updatedBody = updatedBody.replace(
                    testSectionRegex,
                    `${testSectionMarker}\n${testSectionContent}\n${testSectionEnd}`
                  );
                } else {
                  // Add test section before the footer
                  updatedBody = updatedBody.replace(
                    /---\n\n_Updated by automated workflows/,
                    `${testSectionMarker}\n${testSectionContent}\n${testSectionEnd}\n\n---\n\n_Updated by automated workflows`
                  );
                }
                
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: masterComment.id,
                  body: updatedBody
                });
                console.log('Master comment updated with test results');
              } else {
                console.log('Creating new master comment...');
                const newBody = [
                  '<!-- AUTOMATED_TESTS_MASTER_COMMENT -->',
                  '## ü§ñ Automated Tests',
                  '',
                  `${testSectionMarker}`,
                  testSectionContent,
                  `${testSectionEnd}`,
                  '',
                  '---',
                  '',
                  '_Updated by automated workflows ‚Ä¢ Do not edit manually_'
                ].join('\n');
                
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: newBody
                });
                console.log('Master comment created successfully');
              }
            } catch (error) {
              console.error('Error posting comment:', error);
              console.error('Error stack:', error.stack);
              // Don't fail the workflow if comment fails
            }
      
      - name: Check test results and fail if needed
        if: always()
        run: |
          echo "Checking test results..."
          
          # Check state manager tests (run early, no server required)
          if [ "${{ steps.test-state-unit.outcome }}" != "success" ]; then
            echo "‚ùå State manager unit tests failed"
            TESTS_FAILED=true
          fi
          
          if [ "${{ steps.test-state-integration.outcome }}" != "success" ]; then
            echo "‚ùå State manager integration tests failed"
            TESTS_FAILED=true
          fi
          
          # Check draft validation tests (run early, no server required)
          if [ "${{ steps.test-draft.outcome }}" != "success" ]; then
            echo "‚ùå Draft validation tests failed"
            TESTS_FAILED=true
          fi
          
          # Check API client tests (run early, no server required)
          if [ "${{ steps.test-api-client.outcome }}" != "success" ]; then
            echo "‚ùå API client tests failed"
            TESTS_FAILED=true
          fi
          
          # Check SSR integration tests (run early, no server required)
          if [ "${{ steps.test-ssr.outcome }}" != "success" ]; then
            echo "‚ùå SSR integration tests failed"
            TESTS_FAILED=true
          fi
          
          # Check landing page SSR tests (run early, no server required)
          if [ "${{ steps.test-ssr-landing.outcome }}" != "success" ]; then
            echo "‚ùå Landing page SSR tests failed"
            TESTS_FAILED=true
          fi
          
          # Check team session SSR tests (run early, no server required)
          if [ "${{ steps.test-ssr-team.outcome }}" != "success" ]; then
            echo "‚ùå Team session SSR tests failed"
            TESTS_FAILED=true
          fi
          
          # Check formatting utilities tests (run early, no server required)
          if [ "${{ steps.test-formatting.outcome }}" != "success" ]; then
            echo "‚ùå Formatting utilities tests failed"
            TESTS_FAILED=true
          fi
          
          # Check dynamic imports tests (run early, no server required)
          if [ "${{ steps.test-dynamic.outcome }}" != "success" ]; then
            echo "‚ùå Dynamic imports tests failed"
            TESTS_FAILED=true
          fi
          
          # Check dynamic imports E2E tests (run early, no server required)
          if [ "${{ steps.test-dynamic-e2e.outcome }}" != "success" ]; then
            echo "‚ùå Dynamic imports E2E tests failed"
            TESTS_FAILED=true
          fi
          
          # Check performance instrumentation tests (run early, no server required)
          if [ "${{ steps.test-perf-instrumentation.outcome }}" != "success" ]; then
            echo "‚ùå Performance instrumentation tests failed"
            TESTS_FAILED=true
          fi
          
          # Check each test outcome
          if [ "${{ steps.test-api.outcome }}" != "success" ]; then
            echo "‚ùå API endpoint tests failed"
            TESTS_FAILED=true
          fi
          
          if [ "${{ steps.test-db.outcome }}" != "success" ]; then
            echo "‚ùå Database tests failed"
            TESTS_FAILED=true
          fi
          
          if [ "${{ steps.test-frontend.outcome }}" != "success" ]; then
            echo "‚ùå Frontend integration tests failed"
            TESTS_FAILED=true
          fi
          
          if [ "${{ steps.test-flow.outcome }}" != "success" ]; then
            echo "‚ùå Game flow tests failed"
            TESTS_FAILED=true
          fi
          
          if [ "${{ steps.test-nextjs.outcome }}" != "success" ]; then
            echo "‚ùå Next.js routing tests failed"
            TESTS_FAILED=true
          fi
          
          if [ "${{ steps.test-salarycap.outcome }}" != "success" ]; then
            echo "‚ùå Salary cap draft tests failed"
            TESTS_FAILED=true
          fi
          
          if [ "${{ steps.test-performance.outcome }}" != "success" ]; then
            echo "‚ùå Performance benchmarks failed"
            TESTS_FAILED=true
          fi
          
          if [ "${{ steps.test-legacy.outcome }}" != "success" ]; then
            echo "‚ùå Legacy regression tests failed"
            TESTS_FAILED=true
          fi
          
          # Fail the workflow if any tests failed
          if [ "$TESTS_FAILED" = "true" ]; then
            echo "‚ö†Ô∏è One or more test suites failed"
            exit 1
          fi
          
          echo "‚úÖ All tests passed!"
